{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"usm-KbaFqEJm","outputId":"064ef2ab-f728-460e-9253-bd7391969f43"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"TXBA76h9zLfW"},"outputs":[],"source":["dataset='/Users/johnjilima/Desktop/DCIT 316 FINAL PROJECT/bot_detection_data.csv'\n","import pandas as pd\n","df=pd.read_csv(dataset)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"3P2qCdrvzuWY","outputId":"16518156-3d6c-414c-8a27-fce8e2599878"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Username</th>\n","      <th>Tweet</th>\n","      <th>Retweet Count</th>\n","      <th>Mention Count</th>\n","      <th>Follower Count</th>\n","      <th>Verified</th>\n","      <th>Bot Label</th>\n","      <th>Location</th>\n","      <th>Created At</th>\n","      <th>Hashtags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>132131</td>\n","      <td>flong</td>\n","      <td>Station activity person against natural majori...</td>\n","      <td>85</td>\n","      <td>1</td>\n","      <td>2353</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Adkinston</td>\n","      <td>2020-05-11 15:29:50</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>289683</td>\n","      <td>hinesstephanie</td>\n","      <td>Authority research natural life material staff...</td>\n","      <td>55</td>\n","      <td>5</td>\n","      <td>9617</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>Sanderston</td>\n","      <td>2022-11-26 05:18:10</td>\n","      <td>both live</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>779715</td>\n","      <td>roberttran</td>\n","      <td>Manage whose quickly especially foot none to g...</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4363</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>Harrisonfurt</td>\n","      <td>2022-08-08 03:16:54</td>\n","      <td>phone ahead</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>696168</td>\n","      <td>pmason</td>\n","      <td>Just cover eight opportunity strong policy which.</td>\n","      <td>54</td>\n","      <td>5</td>\n","      <td>2242</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Martinezberg</td>\n","      <td>2021-08-14 22:27:05</td>\n","      <td>ever quickly new I</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>704441</td>\n","      <td>noah87</td>\n","      <td>Animal sign six data good or.</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>8438</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Camachoville</td>\n","      <td>2020-04-13 21:24:21</td>\n","      <td>foreign mention</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>491196</td>\n","      <td>uberg</td>\n","      <td>Want but put card direction know miss former h...</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>9911</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Lake Kimberlyburgh</td>\n","      <td>2023-04-20 11:06:26</td>\n","      <td>teach quality ten education any</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>739297</td>\n","      <td>jessicamunoz</td>\n","      <td>Provide whole maybe agree church respond most ...</td>\n","      <td>18</td>\n","      <td>5</td>\n","      <td>9900</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Greenbury</td>\n","      <td>2022-10-18 03:57:35</td>\n","      <td>add walk among believe</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>674475</td>\n","      <td>lynncunningham</td>\n","      <td>Bring different everyone international capital...</td>\n","      <td>43</td>\n","      <td>3</td>\n","      <td>6313</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Deborahfort</td>\n","      <td>2020-07-08 03:54:08</td>\n","      <td>onto admit artist first</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>167081</td>\n","      <td>richardthompson</td>\n","      <td>Than about single generation itself seek sell ...</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>6343</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>Stephenside</td>\n","      <td>2022-03-22 12:13:44</td>\n","      <td>star</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>311204</td>\n","      <td>daniel29</td>\n","      <td>Here morning class various room human true bec...</td>\n","      <td>91</td>\n","      <td>4</td>\n","      <td>4006</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>Novakberg</td>\n","      <td>2022-12-03 06:11:07</td>\n","      <td>home</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["       User ID         Username  \\\n","0       132131            flong   \n","1       289683   hinesstephanie   \n","2       779715       roberttran   \n","3       696168           pmason   \n","4       704441           noah87   \n","...        ...              ...   \n","49995   491196            uberg   \n","49996   739297     jessicamunoz   \n","49997   674475   lynncunningham   \n","49998   167081  richardthompson   \n","49999   311204         daniel29   \n","\n","                                                   Tweet  Retweet Count  \\\n","0      Station activity person against natural majori...             85   \n","1      Authority research natural life material staff...             55   \n","2      Manage whose quickly especially foot none to g...              6   \n","3      Just cover eight opportunity strong policy which.             54   \n","4                          Animal sign six data good or.             26   \n","...                                                  ...            ...   \n","49995  Want but put card direction know miss former h...             64   \n","49996  Provide whole maybe agree church respond most ...             18   \n","49997  Bring different everyone international capital...             43   \n","49998  Than about single generation itself seek sell ...             45   \n","49999  Here morning class various room human true bec...             91   \n","\n","       Mention Count  Follower Count  Verified  Bot Label            Location  \\\n","0                  1            2353     False          1           Adkinston   \n","1                  5            9617      True          0          Sanderston   \n","2                  2            4363      True          0        Harrisonfurt   \n","3                  5            2242      True          1        Martinezberg   \n","4                  3            8438     False          1        Camachoville   \n","...              ...             ...       ...        ...                 ...   \n","49995              0            9911      True          1  Lake Kimberlyburgh   \n","49996              5            9900     False          1           Greenbury   \n","49997              3            6313      True          1         Deborahfort   \n","49998              1            6343     False          0         Stephenside   \n","49999              4            4006     False          0           Novakberg   \n","\n","                Created At                         Hashtags  \n","0      2020-05-11 15:29:50                              NaN  \n","1      2022-11-26 05:18:10                        both live  \n","2      2022-08-08 03:16:54                      phone ahead  \n","3      2021-08-14 22:27:05               ever quickly new I  \n","4      2020-04-13 21:24:21                  foreign mention  \n","...                    ...                              ...  \n","49995  2023-04-20 11:06:26  teach quality ten education any  \n","49996  2022-10-18 03:57:35           add walk among believe  \n","49997  2020-07-08 03:54:08          onto admit artist first  \n","49998  2022-03-22 12:13:44                             star  \n","49999  2022-12-03 06:11:07                             home  \n","\n","[50000 rows x 11 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"FhiyOnB0zvcL"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 11 columns):\n"," #   Column          Non-Null Count  Dtype \n","---  ------          --------------  ----- \n"," 0   User ID         50000 non-null  int64 \n"," 1   Username        50000 non-null  object\n"," 2   Tweet           50000 non-null  object\n"," 3   Retweet Count   50000 non-null  int64 \n"," 4   Mention Count   50000 non-null  int64 \n"," 5   Follower Count  50000 non-null  int64 \n"," 6   Verified        50000 non-null  bool  \n"," 7   Bot Label       50000 non-null  int64 \n"," 8   Location        50000 non-null  object\n"," 9   Created At      50000 non-null  object\n"," 10  Hashtags        41659 non-null  object\n","dtypes: bool(1), int64(5), object(5)\n","memory usage: 3.9+ MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"B0UVIGz5zxEZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["User ID              0\n","Username             0\n","Tweet                0\n","Retweet Count        0\n","Mention Count        0\n","Follower Count       0\n","Verified             0\n","Bot Label            0\n","Location             0\n","Created At           0\n","Hashtags          8341\n","dtype: int64\n"]}],"source":["print(df.isna().sum())"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2cUY9RtYz7d_"},"outputs":[{"name":"stdout","output_type":"stream","text":["User ID              0\n","Username             0\n","Tweet                0\n","Retweet Count        0\n","Mention Count        0\n","Follower Count       0\n","Verified             0\n","Bot Label            0\n","Location             0\n","Created At           0\n","Hashtags          8341\n","dtype: int64\n"]}],"source":["print(df.isnull().sum())"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yKjkn-_x5jxd"},"outputs":[{"name":"stdout","output_type":"stream","text":["0                             No Hashtag\n","1                              both live\n","2                            phone ahead\n","3                     ever quickly new I\n","4                        foreign mention\n","                      ...               \n","49995    teach quality ten education any\n","49996             add walk among believe\n","49997            onto admit artist first\n","49998                               star\n","49999                               home\n","Name: Hashtags, Length: 50000, dtype: object\n"]}],"source":["#adding our own data in missing hastag rows\n","\n","missing_values_mask=df['Hashtags'].isnull()\n","#print(missing_values_mask)\n","df.loc[missing_values_mask,'Hashtags']='No Hashtag'#add No Hashtag at missing cell\n","print(df['Hashtags'])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"sXCA7myL7b1-","outputId":"eaf087b8-9fd8-4082-eff8-564233a011b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["datetime64[ns]\n"]}],"source":["#changing datatype of column 'Created At'\n","\n","# df['Created At']=pd.to_datetime(df['Created At'], format='%Y-%m-%d %H:%M')\n","# print(df['Created At'].dtypes)\n","\n","df['Created At'] = pd.to_datetime(df['Created At'], format='%Y-%m-%d %H:%M:%S')\n","print(df['Created At'].dtypes)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9NuEgxJd9LJ8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Retweet Count</th>\n","      <th>Mention Count</th>\n","      <th>Follower Count</th>\n","      <th>Bot Label</th>\n","      <th>Created At</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>50000.000000</td>\n","      <td>50000.00000</td>\n","      <td>50000.000000</td>\n","      <td>50000.000000</td>\n","      <td>50000.000000</td>\n","      <td>50000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>548890.680540</td>\n","      <td>50.00560</td>\n","      <td>2.513760</td>\n","      <td>4988.602380</td>\n","      <td>0.500360</td>\n","      <td>2021-09-14 06:15:49.674619904</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>100025.000000</td>\n","      <td>0.00000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2020-01-01 00:44:14</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>323524.250000</td>\n","      <td>25.00000</td>\n","      <td>1.000000</td>\n","      <td>2487.750000</td>\n","      <td>0.000000</td>\n","      <td>2020-11-03 09:24:48.500000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>548147.000000</td>\n","      <td>50.00000</td>\n","      <td>3.000000</td>\n","      <td>4991.500000</td>\n","      <td>1.000000</td>\n","      <td>2021-09-16 20:59:38</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>772983.000000</td>\n","      <td>75.00000</td>\n","      <td>4.000000</td>\n","      <td>7471.000000</td>\n","      <td>1.000000</td>\n","      <td>2022-07-23 02:17:01.750000128</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>999995.000000</td>\n","      <td>100.00000</td>\n","      <td>5.000000</td>\n","      <td>10000.000000</td>\n","      <td>1.000000</td>\n","      <td>2023-05-31 07:53:27</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>259756.681425</td>\n","      <td>29.18116</td>\n","      <td>1.708563</td>\n","      <td>2878.742898</td>\n","      <td>0.500005</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             User ID  Retweet Count  Mention Count  Follower Count  \\\n","count   50000.000000    50000.00000   50000.000000    50000.000000   \n","mean   548890.680540       50.00560       2.513760     4988.602380   \n","min    100025.000000        0.00000       0.000000        0.000000   \n","25%    323524.250000       25.00000       1.000000     2487.750000   \n","50%    548147.000000       50.00000       3.000000     4991.500000   \n","75%    772983.000000       75.00000       4.000000     7471.000000   \n","max    999995.000000      100.00000       5.000000    10000.000000   \n","std    259756.681425       29.18116       1.708563     2878.742898   \n","\n","          Bot Label                     Created At  \n","count  50000.000000                          50000  \n","mean       0.500360  2021-09-14 06:15:49.674619904  \n","min        0.000000            2020-01-01 00:44:14  \n","25%        0.000000     2020-11-03 09:24:48.500000  \n","50%        1.000000            2021-09-16 20:59:38  \n","75%        1.000000  2022-07-23 02:17:01.750000128  \n","max        1.000000            2023-05-31 07:53:27  \n","std        0.500005                            NaN  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#summary statistics\n","df.describe()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"NvKnO6g1_f4u"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Username</th>\n","      <th>Tweet</th>\n","      <th>Retweet Count</th>\n","      <th>Mention Count</th>\n","      <th>Follower Count</th>\n","      <th>Verified</th>\n","      <th>Bot Label</th>\n","      <th>Location</th>\n","      <th>Created At</th>\n","      <th>Hashtags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>132131</td>\n","      <td>flong</td>\n","      <td>Station activity person against natural majori...</td>\n","      <td>85</td>\n","      <td>1</td>\n","      <td>2353</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Adkinston</td>\n","      <td>2020-05-11 15:29:50</td>\n","      <td>No Hashtag</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>289683</td>\n","      <td>hinesstephanie</td>\n","      <td>Authority research natural life material staff...</td>\n","      <td>55</td>\n","      <td>5</td>\n","      <td>9617</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>Sanderston</td>\n","      <td>2022-11-26 05:18:10</td>\n","      <td>both live</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>779715</td>\n","      <td>roberttran</td>\n","      <td>Manage whose quickly especially foot none to g...</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4363</td>\n","      <td>True</td>\n","      <td>0</td>\n","      <td>Harrisonfurt</td>\n","      <td>2022-08-08 03:16:54</td>\n","      <td>phone ahead</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>696168</td>\n","      <td>pmason</td>\n","      <td>Just cover eight opportunity strong policy which.</td>\n","      <td>54</td>\n","      <td>5</td>\n","      <td>2242</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Martinezberg</td>\n","      <td>2021-08-14 22:27:05</td>\n","      <td>ever quickly new I</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>704441</td>\n","      <td>noah87</td>\n","      <td>Animal sign six data good or.</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>8438</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Camachoville</td>\n","      <td>2020-04-13 21:24:21</td>\n","      <td>foreign mention</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>491196</td>\n","      <td>uberg</td>\n","      <td>Want but put card direction know miss former h...</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>9911</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Lake Kimberlyburgh</td>\n","      <td>2023-04-20 11:06:26</td>\n","      <td>teach quality ten education any</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>739297</td>\n","      <td>jessicamunoz</td>\n","      <td>Provide whole maybe agree church respond most ...</td>\n","      <td>18</td>\n","      <td>5</td>\n","      <td>9900</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>Greenbury</td>\n","      <td>2022-10-18 03:57:35</td>\n","      <td>add walk among believe</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>674475</td>\n","      <td>lynncunningham</td>\n","      <td>Bring different everyone international capital...</td>\n","      <td>43</td>\n","      <td>3</td>\n","      <td>6313</td>\n","      <td>True</td>\n","      <td>1</td>\n","      <td>Deborahfort</td>\n","      <td>2020-07-08 03:54:08</td>\n","      <td>onto admit artist first</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>167081</td>\n","      <td>richardthompson</td>\n","      <td>Than about single generation itself seek sell ...</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>6343</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>Stephenside</td>\n","      <td>2022-03-22 12:13:44</td>\n","      <td>star</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>311204</td>\n","      <td>daniel29</td>\n","      <td>Here morning class various room human true bec...</td>\n","      <td>91</td>\n","      <td>4</td>\n","      <td>4006</td>\n","      <td>False</td>\n","      <td>0</td>\n","      <td>Novakberg</td>\n","      <td>2022-12-03 06:11:07</td>\n","      <td>home</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["       User ID         Username  \\\n","0       132131            flong   \n","1       289683   hinesstephanie   \n","2       779715       roberttran   \n","3       696168           pmason   \n","4       704441           noah87   \n","...        ...              ...   \n","49995   491196            uberg   \n","49996   739297     jessicamunoz   \n","49997   674475   lynncunningham   \n","49998   167081  richardthompson   \n","49999   311204         daniel29   \n","\n","                                                   Tweet  Retweet Count  \\\n","0      Station activity person against natural majori...             85   \n","1      Authority research natural life material staff...             55   \n","2      Manage whose quickly especially foot none to g...              6   \n","3      Just cover eight opportunity strong policy which.             54   \n","4                          Animal sign six data good or.             26   \n","...                                                  ...            ...   \n","49995  Want but put card direction know miss former h...             64   \n","49996  Provide whole maybe agree church respond most ...             18   \n","49997  Bring different everyone international capital...             43   \n","49998  Than about single generation itself seek sell ...             45   \n","49999  Here morning class various room human true bec...             91   \n","\n","       Mention Count  Follower Count  Verified  Bot Label            Location  \\\n","0                  1            2353     False          1           Adkinston   \n","1                  5            9617      True          0          Sanderston   \n","2                  2            4363      True          0        Harrisonfurt   \n","3                  5            2242      True          1        Martinezberg   \n","4                  3            8438     False          1        Camachoville   \n","...              ...             ...       ...        ...                 ...   \n","49995              0            9911      True          1  Lake Kimberlyburgh   \n","49996              5            9900     False          1           Greenbury   \n","49997              3            6313      True          1         Deborahfort   \n","49998              1            6343     False          0         Stephenside   \n","49999              4            4006     False          0           Novakberg   \n","\n","               Created At                         Hashtags  \n","0     2020-05-11 15:29:50                       No Hashtag  \n","1     2022-11-26 05:18:10                        both live  \n","2     2022-08-08 03:16:54                      phone ahead  \n","3     2021-08-14 22:27:05               ever quickly new I  \n","4     2020-04-13 21:24:21                  foreign mention  \n","...                   ...                              ...  \n","49995 2023-04-20 11:06:26  teach quality ten education any  \n","49996 2022-10-18 03:57:35           add walk among believe  \n","49997 2020-07-08 03:54:08          onto admit artist first  \n","49998 2022-03-22 12:13:44                             star  \n","49999 2022-12-03 06:11:07                             home  \n","\n","[50000 rows x 11 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"U1wdfNehFVU0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Value counts for Username:\n","Username\n","ksmith             21\n","usmith             16\n","msmith             16\n","vmiller            15\n","ismith             13\n","                   ..\n","jessica57           1\n","ggraham             1\n","john93              1\n","gallowaymichael     1\n","daniel29            1\n","Name: count, Length: 40566, dtype: int64\n","\n","Value counts for Tweet:\n","Tweet\n","Station activity person against natural majority none few size expect six marriage.        1\n","Institution second billion over song either arm.                                           1\n","However plan meeting certain dinner card produce wear whether give hour something.         1\n","Total least today until clear nearly economy book single with successful.                  1\n","Full likely beautiful example partner process top catch control natural lead push help.    1\n","                                                                                          ..\n","News society threat positive someone accept stand pressure life so describe pretty.        1\n","Station son hospital figure various visit.                                                 1\n","Friend day town professional simply anyone this or third join somebody current.            1\n","Real chair class order camera tough candidate key nearly car probably hotel.               1\n","Here morning class various room human true because lot send attention.                     1\n","Name: count, Length: 50000, dtype: int64\n","\n","Value counts for Location:\n","Location\n","South Michael        44\n","Lake Michael         40\n","North Jennifer       38\n","North Michael        38\n","Lake David           37\n","                     ..\n","Lake Sarahchester     1\n","Port Tiffanyberg      1\n","New Jasonbury         1\n","Payneshire            1\n","Deborahfort           1\n","Name: count, Length: 25199, dtype: int64\n","\n","Value counts for Created At:\n","Created At\n","2021-02-07 06:50:00    2\n","2020-07-06 03:01:16    2\n","2022-05-19 01:32:03    2\n","2020-06-06 06:17:41    2\n","2020-11-21 14:03:59    2\n","                      ..\n","2021-07-24 23:40:50    1\n","2022-12-05 00:53:29    1\n","2021-03-05 02:48:14    1\n","2021-05-10 02:48:49    1\n","2022-12-03 06:11:07    1\n","Name: count, Length: 49989, dtype: int64\n","\n"]}],"source":["# Assuming you have a DataFrame named 'data' with multiple columns\n","\n","# List of columns to drop\n","columns_to_drop = ['User ID', 'Retweet Count', 'Mention Count','Follower Count','Verified','Bot Label','Hashtags']\n","\n","# Drop the specified columns from the DataFrame\n","data_dropped = df.drop(columns=columns_to_drop)\n","\n","# Get value counts for each remaining column\n","column_counts = {}\n","for column in data_dropped.columns:\n","    column_counts[column] = data_dropped[column].value_counts()\n","\n","# Print the value counts for each column\n","for column, counts in column_counts.items():\n","    print(f\"Value counts for {column}:\")\n","    print(counts)\n","    print()\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"iqHf_Vhi-roX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Hashtags\n","No Hashtag                            8341\n","area                                    21\n","big                                     20\n","treat                                   19\n","ground                                  18\n","                                      ... \n","president conference field process       1\n","market live mouth sit wide               1\n","your five                                1\n","serious not Democrat                     1\n","onto admit artist first                  1\n","Name: count, Length: 34248, dtype: int64\n"]}],"source":["#identify inconsistent formatting for textual(categorical too) columns\n","\n","print(df['Hashtags'].value_counts())\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Gnn5Ramm_TmT"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Filter data based on bot label\n","bot_data = df[df['Bot Label'] == 1]\n","non_bot_data = df[df['Bot Label'] == 0]\n","\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ZY_xD1_0ANur"},"outputs":[{"name":"stdout","output_type":"stream","text":["Follower Count Statistics:\n","Bot Accounts:\n","count    25018.000000\n","mean      4991.944280\n","std       2876.289818\n","min          0.000000\n","25%       2497.000000\n","50%       4978.000000\n","75%       7468.000000\n","max      10000.000000\n","Name: Follower Count, dtype: float64\n","\n","Non-Bot Accounts:\n","count    24982.000000\n","mean      4985.255664\n","std       2881.251104\n","min          0.000000\n","25%       2480.250000\n","50%       5007.500000\n","75%       7472.000000\n","max      10000.000000\n","Name: Follower Count, dtype: float64\n","\n","Retweet Count Statistics:\n","Bot Accounts:\n","count    25018.000000\n","mean        50.042050\n","std         29.171048\n","min          0.000000\n","25%         25.000000\n","50%         50.000000\n","75%         75.000000\n","max        100.000000\n","Name: Retweet Count, dtype: float64\n","\n","Non-Bot Accounts:\n","count    24982.000000\n","mean        49.969098\n","std         29.191822\n","min          0.000000\n","25%         25.000000\n","50%         50.000000\n","75%         75.000000\n","max        100.000000\n","Name: Retweet Count, dtype: float64\n","\n","Mention Count Statistics:\n","Bot Accounts:\n","count    25018.000000\n","mean         2.501959\n","std          1.704641\n","min          0.000000\n","25%          1.000000\n","50%          3.000000\n","75%          4.000000\n","max          5.000000\n","Name: Mention Count, dtype: float64\n","\n","Non-Bot Accounts:\n","count    24982.000000\n","mean         2.525578\n","std          1.712435\n","min          0.000000\n","25%          1.000000\n","50%          3.000000\n","75%          4.000000\n","max          5.000000\n","Name: Mention Count, dtype: float64\n"]}],"source":["bot_followers_stats = bot_data['Follower Count'].describe()\n","non_bot_followers_stats = non_bot_data['Follower Count'].describe()\n","\n","bot_retweet_stats = bot_data['Retweet Count'].describe()\n","non_bot_retweet_stats = non_bot_data['Retweet Count'].describe()\n","\n","bot_mention_stats = bot_data['Mention Count'].describe()\n","non_bot_mention_stats = non_bot_data['Mention Count'].describe()\n","\n","\n","print(\"Follower Count Statistics:\")\n","print(\"Bot Accounts:\")\n","print(bot_followers_stats)\n","print(\"\\nNon-Bot Accounts:\")\n","print(non_bot_followers_stats)\n","\n","print(\"\\nRetweet Count Statistics:\")\n","print(\"Bot Accounts:\")\n","print(bot_retweet_stats)\n","print(\"\\nNon-Bot Accounts:\")\n","print(non_bot_retweet_stats)\n","\n","print(\"\\nMention Count Statistics:\")\n","print(\"Bot Accounts:\")\n","print(bot_mention_stats)\n","print(\"\\nNon-Bot Accounts:\")\n","print(non_bot_mention_stats)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Xhp6OigIMX4E"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Username</th>\n","      <th>Tweet</th>\n","      <th>Retweet Count</th>\n","      <th>Mention Count</th>\n","      <th>Follower Count</th>\n","      <th>Verified</th>\n","      <th>Bot Label</th>\n","      <th>Location</th>\n","      <th>Created At</th>\n","      <th>Hashtags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>132131</td>\n","      <td>flong</td>\n","      <td>Station activity person against natural majori...</td>\n","      <td>85</td>\n","      <td>1</td>\n","      <td>2353</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Adkinston</td>\n","      <td>2020-05-11 15:29:50</td>\n","      <td>No Hashtag</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>289683</td>\n","      <td>hinesstephanie</td>\n","      <td>Authority research natural life material staff...</td>\n","      <td>55</td>\n","      <td>5</td>\n","      <td>9617</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Sanderston</td>\n","      <td>2022-11-26 05:18:10</td>\n","      <td>both live</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>779715</td>\n","      <td>roberttran</td>\n","      <td>Manage whose quickly especially foot none to g...</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4363</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Harrisonfurt</td>\n","      <td>2022-08-08 03:16:54</td>\n","      <td>phone ahead</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>696168</td>\n","      <td>pmason</td>\n","      <td>Just cover eight opportunity strong policy which.</td>\n","      <td>54</td>\n","      <td>5</td>\n","      <td>2242</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Martinezberg</td>\n","      <td>2021-08-14 22:27:05</td>\n","      <td>ever quickly new I</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>704441</td>\n","      <td>noah87</td>\n","      <td>Animal sign six data good or.</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>8438</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Camachoville</td>\n","      <td>2020-04-13 21:24:21</td>\n","      <td>foreign mention</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>491196</td>\n","      <td>uberg</td>\n","      <td>Want but put card direction know miss former h...</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>9911</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Lake Kimberlyburgh</td>\n","      <td>2023-04-20 11:06:26</td>\n","      <td>teach quality ten education any</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>739297</td>\n","      <td>jessicamunoz</td>\n","      <td>Provide whole maybe agree church respond most ...</td>\n","      <td>18</td>\n","      <td>5</td>\n","      <td>9900</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Greenbury</td>\n","      <td>2022-10-18 03:57:35</td>\n","      <td>add walk among believe</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>674475</td>\n","      <td>lynncunningham</td>\n","      <td>Bring different everyone international capital...</td>\n","      <td>43</td>\n","      <td>3</td>\n","      <td>6313</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Deborahfort</td>\n","      <td>2020-07-08 03:54:08</td>\n","      <td>onto admit artist first</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>167081</td>\n","      <td>richardthompson</td>\n","      <td>Than about single generation itself seek sell ...</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>6343</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Stephenside</td>\n","      <td>2022-03-22 12:13:44</td>\n","      <td>star</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>311204</td>\n","      <td>daniel29</td>\n","      <td>Here morning class various room human true bec...</td>\n","      <td>91</td>\n","      <td>4</td>\n","      <td>4006</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Novakberg</td>\n","      <td>2022-12-03 06:11:07</td>\n","      <td>home</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["       User ID         Username  \\\n","0       132131            flong   \n","1       289683   hinesstephanie   \n","2       779715       roberttran   \n","3       696168           pmason   \n","4       704441           noah87   \n","...        ...              ...   \n","49995   491196            uberg   \n","49996   739297     jessicamunoz   \n","49997   674475   lynncunningham   \n","49998   167081  richardthompson   \n","49999   311204         daniel29   \n","\n","                                                   Tweet  Retweet Count  \\\n","0      Station activity person against natural majori...             85   \n","1      Authority research natural life material staff...             55   \n","2      Manage whose quickly especially foot none to g...              6   \n","3      Just cover eight opportunity strong policy which.             54   \n","4                          Animal sign six data good or.             26   \n","...                                                  ...            ...   \n","49995  Want but put card direction know miss former h...             64   \n","49996  Provide whole maybe agree church respond most ...             18   \n","49997  Bring different everyone international capital...             43   \n","49998  Than about single generation itself seek sell ...             45   \n","49999  Here morning class various room human true bec...             91   \n","\n","       Mention Count  Follower Count  Verified  Bot Label            Location  \\\n","0                  1            2353         0          1           Adkinston   \n","1                  5            9617         1          0          Sanderston   \n","2                  2            4363         1          0        Harrisonfurt   \n","3                  5            2242         1          1        Martinezberg   \n","4                  3            8438         0          1        Camachoville   \n","...              ...             ...       ...        ...                 ...   \n","49995              0            9911         1          1  Lake Kimberlyburgh   \n","49996              5            9900         0          1           Greenbury   \n","49997              3            6313         1          1         Deborahfort   \n","49998              1            6343         0          0         Stephenside   \n","49999              4            4006         0          0           Novakberg   \n","\n","               Created At                         Hashtags  \n","0     2020-05-11 15:29:50                       No Hashtag  \n","1     2022-11-26 05:18:10                        both live  \n","2     2022-08-08 03:16:54                      phone ahead  \n","3     2021-08-14 22:27:05               ever quickly new I  \n","4     2020-04-13 21:24:21                  foreign mention  \n","...                   ...                              ...  \n","49995 2023-04-20 11:06:26  teach quality ten education any  \n","49996 2022-10-18 03:57:35           add walk among believe  \n","49997 2020-07-08 03:54:08          onto admit artist first  \n","49998 2022-03-22 12:13:44                             star  \n","49999 2022-12-03 06:11:07                             home  \n","\n","[50000 rows x 11 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["#label encoding of boolean data\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder=LabelEncoder()\n","df['Verified']=label_encoder.fit_transform(df['Verified'])\n","df"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"RmN5Ezpb4kLD"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>User ID</th>\n","      <th>Username</th>\n","      <th>Tweet</th>\n","      <th>Retweet Count</th>\n","      <th>Mention Count</th>\n","      <th>Follower Count</th>\n","      <th>Verified</th>\n","      <th>Bot Label</th>\n","      <th>Location</th>\n","      <th>Created At</th>\n","      <th>Hashtags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>132131</td>\n","      <td>flong</td>\n","      <td>Station activity person against natural majori...</td>\n","      <td>85</td>\n","      <td>1</td>\n","      <td>2353</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Adkinston</td>\n","      <td>2020-05-11 15:29:50</td>\n","      <td>No Hashtag</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>289683</td>\n","      <td>hinesstephanie</td>\n","      <td>Authority research natural life material staff...</td>\n","      <td>55</td>\n","      <td>5</td>\n","      <td>9617</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Sanderston</td>\n","      <td>2022-11-26 05:18:10</td>\n","      <td>both live</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>779715</td>\n","      <td>roberttran</td>\n","      <td>Manage whose quickly especially foot none to g...</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>4363</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>Harrisonfurt</td>\n","      <td>2022-08-08 03:16:54</td>\n","      <td>phone ahead</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>696168</td>\n","      <td>pmason</td>\n","      <td>Just cover eight opportunity strong policy which.</td>\n","      <td>54</td>\n","      <td>5</td>\n","      <td>2242</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Martinezberg</td>\n","      <td>2021-08-14 22:27:05</td>\n","      <td>ever quickly new I</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>704441</td>\n","      <td>noah87</td>\n","      <td>Animal sign six data good or.</td>\n","      <td>26</td>\n","      <td>3</td>\n","      <td>8438</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Camachoville</td>\n","      <td>2020-04-13 21:24:21</td>\n","      <td>foreign mention</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>49995</th>\n","      <td>491196</td>\n","      <td>uberg</td>\n","      <td>Want but put card direction know miss former h...</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>9911</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Lake Kimberlyburgh</td>\n","      <td>2023-04-20 11:06:26</td>\n","      <td>teach quality ten education any</td>\n","    </tr>\n","    <tr>\n","      <th>49996</th>\n","      <td>739297</td>\n","      <td>jessicamunoz</td>\n","      <td>Provide whole maybe agree church respond most ...</td>\n","      <td>18</td>\n","      <td>5</td>\n","      <td>9900</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>Greenbury</td>\n","      <td>2022-10-18 03:57:35</td>\n","      <td>add walk among believe</td>\n","    </tr>\n","    <tr>\n","      <th>49997</th>\n","      <td>674475</td>\n","      <td>lynncunningham</td>\n","      <td>Bring different everyone international capital...</td>\n","      <td>43</td>\n","      <td>3</td>\n","      <td>6313</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Deborahfort</td>\n","      <td>2020-07-08 03:54:08</td>\n","      <td>onto admit artist first</td>\n","    </tr>\n","    <tr>\n","      <th>49998</th>\n","      <td>167081</td>\n","      <td>richardthompson</td>\n","      <td>Than about single generation itself seek sell ...</td>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>6343</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Stephenside</td>\n","      <td>2022-03-22 12:13:44</td>\n","      <td>star</td>\n","    </tr>\n","    <tr>\n","      <th>49999</th>\n","      <td>311204</td>\n","      <td>daniel29</td>\n","      <td>Here morning class various room human true bec...</td>\n","      <td>91</td>\n","      <td>4</td>\n","      <td>4006</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Novakberg</td>\n","      <td>2022-12-03 06:11:07</td>\n","      <td>home</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>50000 rows Ã— 11 columns</p>\n","</div>"],"text/plain":["       User ID         Username  \\\n","0       132131            flong   \n","1       289683   hinesstephanie   \n","2       779715       roberttran   \n","3       696168           pmason   \n","4       704441           noah87   \n","...        ...              ...   \n","49995   491196            uberg   \n","49996   739297     jessicamunoz   \n","49997   674475   lynncunningham   \n","49998   167081  richardthompson   \n","49999   311204         daniel29   \n","\n","                                                   Tweet  Retweet Count  \\\n","0      Station activity person against natural majori...             85   \n","1      Authority research natural life material staff...             55   \n","2      Manage whose quickly especially foot none to g...              6   \n","3      Just cover eight opportunity strong policy which.             54   \n","4                          Animal sign six data good or.             26   \n","...                                                  ...            ...   \n","49995  Want but put card direction know miss former h...             64   \n","49996  Provide whole maybe agree church respond most ...             18   \n","49997  Bring different everyone international capital...             43   \n","49998  Than about single generation itself seek sell ...             45   \n","49999  Here morning class various room human true bec...             91   \n","\n","       Mention Count  Follower Count  Verified  Bot Label            Location  \\\n","0                  1            2353         0          1           Adkinston   \n","1                  5            9617         1          0          Sanderston   \n","2                  2            4363         1          0        Harrisonfurt   \n","3                  5            2242         1          1        Martinezberg   \n","4                  3            8438         0          1        Camachoville   \n","...              ...             ...       ...        ...                 ...   \n","49995              0            9911         1          1  Lake Kimberlyburgh   \n","49996              5            9900         0          1           Greenbury   \n","49997              3            6313         1          1         Deborahfort   \n","49998              1            6343         0          0         Stephenside   \n","49999              4            4006         0          0           Novakberg   \n","\n","               Created At                         Hashtags  \n","0     2020-05-11 15:29:50                       No Hashtag  \n","1     2022-11-26 05:18:10                        both live  \n","2     2022-08-08 03:16:54                      phone ahead  \n","3     2021-08-14 22:27:05               ever quickly new I  \n","4     2020-04-13 21:24:21                  foreign mention  \n","...                   ...                              ...  \n","49995 2023-04-20 11:06:26  teach quality ten education any  \n","49996 2022-10-18 03:57:35           add walk among believe  \n","49997 2020-07-08 03:54:08          onto admit artist first  \n","49998 2022-03-22 12:13:44                             star  \n","49999 2022-12-03 06:11:07                             home  \n","\n","[50000 rows x 11 columns]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"u9Nwt9Ku8sno"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 50000 entries, 0 to 49999\n","Data columns (total 11 columns):\n"," #   Column          Non-Null Count  Dtype         \n","---  ------          --------------  -----         \n"," 0   User ID         50000 non-null  int64         \n"," 1   Username        50000 non-null  object        \n"," 2   Tweet           50000 non-null  object        \n"," 3   Retweet Count   50000 non-null  int64         \n"," 4   Mention Count   50000 non-null  int64         \n"," 5   Follower Count  50000 non-null  int64         \n"," 6   Verified        50000 non-null  int64         \n"," 7   Bot Label       50000 non-null  int64         \n"," 8   Location        50000 non-null  object        \n"," 9   Created At      50000 non-null  datetime64[ns]\n"," 10  Hashtags        50000 non-null  object        \n","dtypes: datetime64[ns](1), int64(6), object(4)\n","memory usage: 4.2+ MB\n"]}],"source":["df.info()"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"RW1zQVnI3jjO"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack\n","# Combine all text columns into a single Series\n","text_data = df['Tweet'] + ' ' + df['Username'] + ' ' + df['Hashtags']+' '+df['Location']\n","\n","# Text vectorization using TF-IDF (sparse representation)\n","vectorizer = TfidfVectorizer()\n","text_sparse = vectorizer.fit_transform(text_data)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"45asjs5A-qsb"},"outputs":[],"source":["# Combine text features with additional features\n","\n","additional_features = df[['Retweet Count', 'Verified','Mention Count','Follower Count','Created At']]"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"ZEJTUy2V-QW3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/3c/mnf59n6d2tj10v6pg4c26tfc0000gp/T/ipykernel_5807/1003833889.py:1: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  additional_features['Created At'] = additional_features['Created At'].astype(int)  # Convert to Unix timestamp\n"]}],"source":["additional_features['Created At'] = additional_features['Created At'].astype(int)  # Convert to Unix timestamp\n","additional_features = additional_features.astype('float64')  # Convert to float64\n","text_sparse = text_sparse.astype('float64')"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"lf1Tlb1M9c4p"},"outputs":[],"source":["\n","\n","combined_sparse = hstack((text_sparse, additional_features))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"29F_wEB34yfz","outputId":"895974f8-887e-4b99-b5e1-f362f5aaf9ae"},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div>"],"text/plain":["GradientBoostingClassifier()"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.model_selection import train_test_split\n","\n","# from sklearn.ensemble import GradientBoostingClassifier\n","# from sklearn.metrics import classification_report\n","\n","\n","# # Split the data into training and testing sets\n","# X_train, X_test, y_train, y_test = train_test_split(combined_sparse, df['Bot Label'], test_size=0.2, random_state=42)\n","\n","# # Create an instance of the Random Forest classifier\n","# # rf_classifier = RandomForestClassifier()\n","\n","# # # Train the Random Forest classifier\n","# # rf_classifier.fit(X_train, y_train)\n","\n","\n","# # Create an instance of the Gradient Boosting classifier\n","# gb_classifier = GradientBoostingClassifier()\n","\n","# # Train the Gradient Boosting classifier\n","# gb_classifier.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","500/500 [==============================] - 10s 20ms/step - loss: 0.7130 - accuracy: 0.5004 - val_loss: 0.6950 - val_accuracy: 0.4921\n","Epoch 2/10\n","500/500 [==============================] - 8s 15ms/step - loss: 0.5612 - accuracy: 0.7147 - val_loss: 0.7289 - val_accuracy: 0.4978\n","Epoch 3/10\n","500/500 [==============================] - 8s 15ms/step - loss: 0.2464 - accuracy: 0.8992 - val_loss: 0.8703 - val_accuracy: 0.4933\n","Epoch 4/10\n","500/500 [==============================] - 8s 16ms/step - loss: 0.0844 - accuracy: 0.9683 - val_loss: 1.0919 - val_accuracy: 0.4927\n","Epoch 5/10\n","500/500 [==============================] - 8s 16ms/step - loss: 0.0372 - accuracy: 0.9863 - val_loss: 1.3945 - val_accuracy: 0.4909\n","Epoch 6/10\n","500/500 [==============================] - 8s 15ms/step - loss: 0.0213 - accuracy: 0.9923 - val_loss: 1.5564 - val_accuracy: 0.4950\n","Epoch 7/10\n","500/500 [==============================] - 8s 16ms/step - loss: 0.0133 - accuracy: 0.9953 - val_loss: 1.7749 - val_accuracy: 0.4920\n","Epoch 8/10\n","500/500 [==============================] - 8s 17ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 1.8737 - val_accuracy: 0.4889\n","Epoch 9/10\n","500/500 [==============================] - 8s 15ms/step - loss: 0.0093 - accuracy: 0.9970 - val_loss: 1.8827 - val_accuracy: 0.4840\n","Epoch 10/10\n","500/500 [==============================] - 8s 15ms/step - loss: 0.0072 - accuracy: 0.9973 - val_loss: 1.9980 - val_accuracy: 0.4890\n","313/313 [==============================] - 1s 3ms/step - loss: 2.3764 - accuracy: 0.5133\n","Test accuracy: 0.5133\n"]}],"source":["from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import numpy as np\n","\n","X_train, X_test, y_train, y_test = train_test_split(combined_sparse, df['Bot Label'], test_size=0.2, random_state=42)\n","\n","# Standardize the input features\n","scaler = StandardScaler(with_mean=False)\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Manually create a validation split\n","split_index = int(X_train_scaled.shape[0] * 0.8)\n","X_train_split, X_val_split = X_train_scaled[:split_index], X_train_scaled[split_index:]\n","y_train_split, y_val_split = y_train[:split_index], y_train[split_index:]\n","\n","# Convert sparse matrices to dense arrays\n","X_train_dense = X_train_split.toarray()\n","X_val_dense = X_val_split.toarray()\n","\n","# Create a neural network model\n","model = Sequential()\n","model.add(Dense(128, activation='relu', input_shape=(X_train_dense.shape[1],)))\n","model.add(Dropout(0.3))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.3))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_dense, y_train_split, epochs=10, batch_size=64, validation_data=(X_val_dense, y_val_split))\n","\n","# Convert the test data to dense array\n","X_test_dense = X_test_scaled.toarray()\n","\n","# Evaluate the model on the test set\n","loss, accuracy = model.evaluate(X_test_dense, y_test)\n","print(f\"Test accuracy: {accuracy:.4f}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
